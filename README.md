# supervised-learning-in-latent-space
Using variational autoencoders to create compressed representation of data, so that a simple neural net can make predictions on them (inspired by world models).

Traditional way of using neural networks seems to be pretty limited. The cool thing about using latent spaces for image prediction is that there's what could be defined as a conceptual representation of the object; this is testified by the fact that, despite changing the colors of the MNIST images, the network is still able to predict the correct number.
